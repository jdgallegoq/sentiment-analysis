{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# check if gpu is avaibale\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "torch.device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/transformers/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/transformers/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# download pretrained model\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "print(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer secuence: [101, 3958, 27227, 2001, 1037, 13997, 11510, 102, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/transformers/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# example of tokenizer encoding\n",
    "text = \"Jim Henson was a puppeteer\"\n",
    "\n",
    "sent_id = tokenizer.encode(\n",
    "    text,\n",
    "    add_special_tokens=True, # add [CLS] and [SEP] tokens\n",
    "    max_length=10, # define max secuence lenght\n",
    "    truncation=True,\n",
    "    pad_to_max_length='right' # pad tokens to the right\n",
    ")\n",
    "print(\"Integer secuence: {}\".format(sent_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 101: [CLS] token\n",
    "* 102: [SEP] token\n",
    "* 0: [PAD] token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text: ['[CLS]', 'jim', 'henson', 'was', 'a', 'puppet', '##eer', '[SEP]', '[PAD]', '[PAD]']\n",
      "Decoded sequence: [CLS] jim henson was a puppeteer [SEP] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# convert integers back to sequence\n",
    "print(\"Tokenized text: {}\".format(tokenizer.convert_ids_to_tokens(sent_id)))\n",
    "\n",
    "# Decode to see the original not tokenized\n",
    "print(\"Decoded sequence: {}\".format(tokenizer.decode(sent_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  3958, 27227,  2001,  1037, 13997, 11510,   102,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "# understand inputs and outputs\n",
    "att_mask = [int(tok>0) for tok in sent_id]\n",
    "# convert to tensors ids and attention mask\n",
    "sent_id = torch.tensor(sent_id)\n",
    "att_mask = torch.tensor(att_mask)\n",
    "\n",
    "# reshape in form of (batch, text_lenght)\n",
    "sent_id = sent_id.unsqueeze(0)\n",
    "att_mask = att_mask.unsqueeze(0)\n",
    "\n",
    "# reshaped tensor\n",
    "print(sent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768]) torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# pass tensors to BERT\n",
    "outputs = bert(sent_id, attention_mask=att_mask)\n",
    "# hidden states at eacht timestep\n",
    "all_hidden_states = outputs[0]\n",
    "# hidden states at first timestep ([CLS] token)\n",
    "cls_hidden_state = outputs[1]\n",
    "\n",
    "print(all_hidden_states.shape, cls_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('..')\n",
    "from utils.s3_class import S3Functions\n",
    "\n",
    "s3_funcs = S3Functions(bucket_name='jdgallegoq-sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 15 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   tweet_id                      14640 non-null  int64  \n",
      " 1   airline_sentiment             14640 non-null  object \n",
      " 2   airline_sentiment_confidence  14640 non-null  float64\n",
      " 3   negativereason                9178 non-null   object \n",
      " 4   negativereason_confidence     10522 non-null  float64\n",
      " 5   airline                       14640 non-null  object \n",
      " 6   airline_sentiment_gold        40 non-null     object \n",
      " 7   name                          14640 non-null  object \n",
      " 8   negativereason_gold           32 non-null     object \n",
      " 9   retweet_count                 14640 non-null  int64  \n",
      " 10  text                          14640 non-null  object \n",
      " 11  tweet_coord                   1019 non-null   object \n",
      " 12  tweet_created                 14640 non-null  object \n",
      " 13  tweet_location                9907 non-null   object \n",
      " 14  user_timezone                 9820 non-null   object \n",
      "dtypes: float64(2), int64(2), object(11)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "df = pd.read_csv(s3_funcs.read_object(key='bert/Tweets.csv'))\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative    9178\n",
      "neutral     3099\n",
      "positive    2363\n",
      "Name: airline_sentiment, dtype: int64\n",
      "negative    0.626913\n",
      "neutral     0.211680\n",
      "positive    0.161407\n",
      "Name: airline_sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# targets distribution\n",
    "print(df.airline_sentiment.value_counts())\n",
    "# by proportion\n",
    "print(df.airline_sentiment.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df.airline_sentiment.value_counts().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning\n",
    "def text_preprocess(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove mentions\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    # remove hashtags\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    # remove links\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # split tokens to remove extra spaces\n",
    "    tokens = text.split()\n",
    "\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applyt function and clean text\n",
    "df['clean_text'] = df['text'].apply(text_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate text and labels\n",
    "text = df['clean_text'].values\n",
    "labels = df['airline_sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative' 'neutral' 'positive']\n",
      "[1 2 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# preprare output\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "\n",
    "# classes\n",
    "print(le.classes_)\n",
    "# labels\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# preprare inputs\n",
    "num_words = [len(t.split()) for t in text]\n",
    "plt.hist(num_words, bins=30)\n",
    "plt.title('Histogram text lenghts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define max lenght looking to have max num of tweets\n",
    "max_len = 26 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14640 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable /opt/homebrew/Caskroom/miniforge/base/envs/transformers/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 14640/14640 [00:01<00:00, 12478.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# sent ids list\n",
    "sent_id = []\n",
    "for i in tqdm(range(len(text))):\n",
    "    encoded_sent = tokenizer.encode(\n",
    "        text[i],\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        pad_to_max_length='right'\n",
    "    )\n",
    "    sent_id.append(encoded_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14640/14640 [00:00<00:00, 515927.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# create attention masks\n",
    "attention_masks = []\n",
    "for sent in tqdm(sent_id):\n",
    "    att_mask = [int(tok>0) for tok in sent]\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    sent_id,\n",
    "    labels,\n",
    "    random_state=5132024,\n",
    "    test_size=0.1,\n",
    "    stratify=labels\n",
    ")\n",
    "# repeat for masks\n",
    "train_masks, val_masks, _ ,_ = train_test_split(\n",
    "    attention_masks,\n",
    "    labels,\n",
    "    random_state=5132024,\n",
    "    test_size=0.1,\n",
    "    stratify=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "# inputs\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "# labels\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "# masks\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (\n",
    "    TensorDataset,\n",
    "    DataLoader,\n",
    "    RandomSampler,\n",
    "    SequentialSampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch size. recommended {16, 32}\n",
    "batch_size = 32\n",
    "\n",
    "# dataloader for train\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "# define sampler.\n",
    "# Random: smaples randomly\n",
    "# Sequential: samples sequentially, always in the same order\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# represent all of it in an iterator\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# same for val\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "# define sampler.\n",
    "# Random: smaples randomly\n",
    "# Sequential: samples sequentially, always in the same order\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "# represent all of it in an iterator\n",
    "val_dataloader = DataLoader(\n",
    "    val_data,\n",
    "    sampler=val_sampler,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# define iterator\n",
    "#iterator = iter(train_dataloader)\n",
    "# loads batch data\n",
    "#sent_id, mask, target = next(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model finetunning\n",
    "Finetune Head only. Steps:\n",
    "1. Turn off gradients (cause we're going to finetune Head only)\n",
    "2. Define model architecture\n",
    "3. Define optimizer and loss\n",
    "4. Define train and evaluate.\n",
    "5. Train model\n",
    "6. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. turn off gradients\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. define model architecture.\n",
    "class classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(classifier, self).__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "        # dense 1\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        # dense 2 (output layer, 3 cells as 3 labels avaiable)\n",
    "        self.fc2 = nn.Linear(512, 3)\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        # activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        # softmax activation (to get probas)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # define forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        # pass inputs to the models\n",
    "        all_hidden_states, cls_hidden_state = self.bert(\n",
    "            sent_id,\n",
    "            attention_mask=mask,\n",
    "            return_dict=False\n",
    "        )\n",
    "        # pass CLS hidden state to dense layer\n",
    "        x = self.fc1(cls_hidden_state)\n",
    "        # apply ReLU\n",
    "        x = self.relu(x)\n",
    "        # apply dropout\n",
    "        x = self.dropout(x)\n",
    "        # pass input to the output layer\n",
    "        x = self.fc2(x)\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = classifier(bert)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395267"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count model trainable params\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer and loss\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights:  [0.53170625 1.57470152 2.06517139]\n"
     ]
    }
   ],
   "source": [
    "# as classes are unbalanced, then calculate weights for each class\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "print(\"Class weights: \", class_weights)\n",
    "\n",
    "# then pass it as tensors\n",
    "weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# transfer to gpu is available\n",
    "if torch.cuda.is_available():\n",
    "    weights = weights.to(device)\n",
    "\n",
    "# cross-entropy\n",
    "cross_entropy = nn.NLLLoss(weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model train\n",
    "def train():\n",
    "    print(\"\\nTraining\")\n",
    "    # set model to training phase\n",
    "    model.train()\n",
    "    # initialize loss and accuracy to 0\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    # list for preds\n",
    "    total_preds = []\n",
    "\n",
    "    # loop over train batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # push batch to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "        else:\n",
    "            batch = tuple(t for t in batch)\n",
    "        \n",
    "        # unpack batch tensors\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # reset gradient optimization. Pytorch does not do it automatically\n",
    "        model.zero_grad()\n",
    "        # forward\n",
    "        preds = model(sent_id, mask)\n",
    "        # loss\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        # accumulate it. Access the tensor value by doing .item()\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # perform backward to calculate gradients\n",
    "        loss.backward()\n",
    "        # optimize gradients\n",
    "        optimizer.step()\n",
    "        #if torch.cuda.is_available():\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        \n",
    "        # accumulate model preds\n",
    "        total_preds.append(preds)\n",
    "    \n",
    "    # compute training loss of a epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    # rehsape preds [batches, batch_size, classes] -> [samples, classes]\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "def evaluate():\n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "    # set the model on evaluating phase - dropout layers deactivated\n",
    "    model.eval()\n",
    "\n",
    "    # initialize loss and accuracy\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # create an empty list to save model preds\n",
    "    total_preds = []\n",
    "\n",
    "    # loop over batches\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "        if torch.cuda.is_available():\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "        else:\n",
    "            batch = tuple(t for t in batch)\n",
    "\n",
    "        # unpack tensors\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # forward\n",
    "            preds = model(sent_id, mask)\n",
    "            # compute val loss\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            # accumulate loss\n",
    "            total_loss = total_loss + loss.item()\n",
    "            # pass preds to cpu\n",
    "            #if torch.cuda.is_available():\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            # accumulate preds at each batch\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute val loss of an epoch\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "    # reshape preds\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "....... epoch 1 / 5 .......\n",
      "\n",
      "Training\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.980\n",
      "Validation Loss: 0.963\n",
      "\n",
      "....... epoch 2 / 5 .......\n",
      "\n",
      "Training\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.843\n",
      "Validation Loss: 0.824\n",
      "\n",
      "....... epoch 3 / 5 .......\n",
      "\n",
      "Training\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.790\n",
      "Validation Loss: 0.736\n",
      "\n",
      "....... epoch 4 / 5 .......\n",
      "\n",
      "Training\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.767\n",
      "Validation Loss: 0.688\n",
      "\n",
      "....... epoch 5 / 5 .......\n",
      "\n",
      "Training\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.761\n",
      "Validation Loss: 0.704\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# train process in action\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty list for train and val losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('\\n....... epoch {:} / {:} .......'.format(epoch + 1, epochs))\n",
    "\n",
    "    # train\n",
    "    train_loss, _ = train()\n",
    "    # eval\n",
    "    val_loss, _ = evaluate()\n",
    "\n",
    "    # save best model\n",
    "    if val_loss < best_valid_loss:\n",
    "        best_valid_loss = val_loss\n",
    "        torch.save(model.state_dict(), '../module/models/bert_save_weights.pt')\n",
    "    \n",
    "    # accumulate training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {val_loss:.3f}')\n",
    "\n",
    "print('\\nTraining complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model weights\n",
    "path = \"../module/models/bert_save_weights.pt\"\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    }
   ],
   "source": [
    "valid_loss, preds = evaluate()\n",
    "print(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert\n",
    "y_pred = np.argmax(preds, axis=1)\n",
    "y_true = val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.80      0.83       918\n",
      "           1       0.57      0.49      0.53       310\n",
      "           2       0.54      0.80      0.65       236\n",
      "\n",
      "    accuracy                           0.73      1464\n",
      "   macro avg       0.66      0.70      0.67      1464\n",
      "weighted avg       0.75      0.73      0.74      1464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
